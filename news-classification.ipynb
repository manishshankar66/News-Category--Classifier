{"cells":[{"metadata":{"_uuid":"fda0c01cc411750003d94b16edb3c3bd7b24e5d4"},"cell_type":"markdown","source":"## Preprocessing"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir('../input'))\n#Read the data in dataframe\ndf = pd.read_json('../input/news-category-dataset/News_Category_Dataset.json', lines=True)\nprint(df.shape)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"faf01140f6aeeefff8f7205141cc774afa7d4972"},"cell_type":"markdown","source":"## Analysis"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"df.groupby(by='category')['category'].count().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69a0a1a73108ad5e61939677b127d17b2fe0745a"},"cell_type":"code","source":"#Merge `THE WORLDPOST` and `WORLDPOST` into single category\ndf.category = df.category.map(lambda x: \"WORLDPOST\" if x == \"THE WORLDPOST\" else x)\ndf.groupby(by='category')['category'].count().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"80b90853d72ff22e7cd91f0b5ae030da900cb1c6"},"cell_type":"code","source":"#Number of articles published by month\ndf['date'] = pd.to_datetime(df['date'], errors='coerce')\ndf.groupby(pd.Grouper(key='date', freq='M'))['date'].count().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4351cf102c239bfe1d38901aa25d6b5d3b1c515b"},"cell_type":"code","source":"#Average articles per month\ndf.groupby(pd.Grouper(key='date', freq='M'))['date'].count().mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d10e806b6536fa944539a80c93d0219a1031d870"},"cell_type":"code","source":"#Popular category per month\ndf.groupby(pd.Grouper(key='date', freq='M'))['category'].agg(lambda x:x.value_counts().index[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"144a3262cbf387b2e33288859a72b26d55bd14eb"},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true,"_uuid":"adbbcf4c2efc1ef22bfc91092befc04d2d1345f0"},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n\n#Tokenize headline\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(df.headline)\nX = tokenizer.texts_to_sequences(df.headline)\ndf['words'] = X\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dafaa31d90259b1ed0a861d9792b42fbdfd26690"},"cell_type":"code","source":"#Use GLOVE pretrained word-embeddings\nEMBEDDING_DIMENSION=100\nembeddings_index = {}\nf = open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12425422d91bacfad5af715b52925505b106a03c"},"cell_type":"code","source":"#Create a weight matrix for words in training docs\nword_index = tokenizer.word_index\nembedding_matrix = np.zeros((len(word_index) + 1, 100))\n\nfor word, index in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16fd4e4ee7af96c3fb31a773b5dc5e153a778aad"},"cell_type":"code","source":"from keras.layers.embeddings import Embedding\nfrom keras.initializers import Constant\n\n#Create embedding layer from embedding matrix\nembedding_layer = Embedding(len(word_index)+1, EMBEDDING_DIMENSION,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=50, trainable=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6778ee61f95d0729a653789eaf85a394381875e8"},"cell_type":"code","source":"from keras.utils import np_utils\nfrom keras.preprocessing import sequence\nfrom sklearn.model_selection import train_test_split\n\n#Prepare training and test data\nX = np.array(list(sequence.pad_sequences(df.words, maxlen=50)))\n\ncategory_dict = dict((i,k) for k,i in enumerate(list(df.groupby('category').groups.keys())))\ndf['labels'] = df['category'].apply(lambda x: category_dict[x])\nY = np_utils.to_categorical(list(df.labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9980e7ba60689b4be8f26bbf1473df72e86619e5"},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout, BatchNormalization, Flatten\n\n#RNN with LSTM\nmodel = Sequential()\n\nmodel.add(embedding_layer)\nmodel.add(LSTM(300, dropout=0.25, recurrent_dropout=0.25))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(256, activation='sigmoid'))\nmodel.add(Dropout(0.25))\nmodel.add(BatchNormalization())\nmodel.add(Dense(len(category_dict), activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88d13626b977d274932b7a0be3c101b9404ab5c2","scrolled":true},"cell_type":"code","source":"model_history = model.fit(X, Y, batch_size=128, validation_split=0.4, epochs=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9dccd1e3d4bc52f38fc979867cd2503afacad4e9"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nacc = model_history.history['acc']\nval_acc = model_history.history['val_acc']\nloss = model_history.history['loss']\nval_loss = model_history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()\n\nplt.figure()\nplt.title('Training and validation loss')\nplt.plot(epochs, loss, 'red', label='Training loss')\nplt.plot(epochs, val_loss, 'blue', label='Validation loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a695ae441ba9739059d39f753d31993e18980c44"},"cell_type":"markdown","source":"### Using Short Description to train model"},{"metadata":{"trusted":true,"_uuid":"f381cf7f99253060161c464478597ec6efc34beb"},"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(df.short_description)\nX = tokenizer.texts_to_sequences(df.short_description)\ndf['short_description_tokenize'] = X\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64eede463d8bc0b8e4ead69bd5733b6aa58602e0"},"cell_type":"code","source":"word_index = tokenizer.word_index\nembedding_matrix = np.zeros((len(word_index) + 1, 100))\n\nfor word, index in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8cd5353cbf345c7a6e0d46ef821d8faceddbf362"},"cell_type":"code","source":"embedding_layer = Embedding(len(word_index)+1, EMBEDDING_DIMENSION,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=50, trainable=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e74b1d8af801aae2ab1fc528736d5f5b3883dac0"},"cell_type":"code","source":"X = np.array(list(sequence.pad_sequences(df.short_description_tokenize, maxlen=50)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1e6fe1fd16e6317d9f19fc257711aa031fe6ecb"},"cell_type":"code","source":"model = Sequential()\n\nmodel.add(embedding_layer)\nmodel.add(LSTM(300, dropout=0.25, recurrent_dropout=0.25))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(256, activation='sigmoid'))\nmodel.add(Dropout(0.25))\nmodel.add(BatchNormalization())\nmodel.add(Dense(len(category_dict), activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6979c318ac585627df756d58a381c78f772f996"},"cell_type":"code","source":"model_history = model.fit(X, Y, batch_size=128, validation_split=0.3, epochs=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3dd494f0e7b76f1bd9fc2e62294f30c5cf0be21"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nacc = model_history.history['acc']\nval_acc = model_history.history['val_acc']\nloss = model_history.history['loss']\nval_loss = model_history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()\n\nplt.figure()\nplt.title('Training and validation loss')\nplt.plot(epochs, loss, 'red', label='Training loss')\nplt.plot(epochs, val_loss, 'blue', label='Validation loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb07a6e73e65c298dcf8c421fa24258419cff00e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}